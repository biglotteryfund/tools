#!/usr/bin/env node
'use strict';
const path = require('path');
const moment = require('moment');
const fs = require('fs');
const rp = require('request-promise-native');
const pSettle = require('p-settle');
const jsdom = require('jsdom');
const { JSDOM } = jsdom;
const pretty = require('pretty');
const absolution = require('absolution');
const he = require('he');
const { forEach, difference } = require('lodash');
const urls = require('./programmes.json');

const argv = require('yargs')
    .alias('s', 'scrape')
    .describe('s', 'Scrape new data for the listed URLs')
    .alias('p', 'parse')
    .describe('p', 'Parse content HTML from existing scraped data')
    .alias('u', 'url')
    .describe('u', 'Specify a URL to scrape/parse rather than looping the default list')
    .help('h')
    .alias('h', 'help').argv;

if (!argv.s && !argv.p) {
    console.log('Please specify either --scrape or --parse');
    process.exit(1);
}

const scrapedDataDir = path.join(__dirname, 'content/scraped/');
const cleanedDataDir = path.join(__dirname, 'content/cleaned/');

const urlToFilename = url => {
    return url.replace('https://31.221.8.237/', '').replace(/\//g, '-');
};
let numErrs = 0;
let scrapeUrl = urlConf => {
    return rp({
        url: urlConf.url,
        strictSSL: false,
        jar: true,
        resolveWithFullResponse: true,
        maxRedirects: 2,
        timeout: 1200000
    }).then(response => {
        return {
            urlConf: urlConf,
            response: response
        };
    });
};

let storeContent = (responses) => {

    let languages = {};
    let canonicalUrl = responses.find(r => r.isFulfilled && r.value.urlConf.lang === 'en').value.urlConf.url;

    // fetch english and welsh pages
    responses.forEach(responseData => {

        if (responseData.isRejected) {
            console.log('Failed to parse URL', responseData.reason.message);
        }

        // some promises may fail (eg. no Welsh URL exists)
        if (responseData.isFulfilled) {
            let response = responseData.value.response;
            let urlConf = responseData.value.urlConf;
            let body = response.body;

            // convert links/images etc to point directly to live env
            body = absolution(body, 'https://www.biglotteryfund.org.uk');

            // parse the DOM
            let dom = new JSDOM(body);

            // remove redundant ASP viewstate
            let viewState = dom.window.document.getElementById('__VIEWSTATE');
            if (viewState) {
                viewState.parentNode.removeChild(viewState);
            }

            // clean up HTML
            let html = dom.window.document.body.innerHTML;

            // remove junk strings
            html = html.replace(/ {2,}/, '')
                .replace('\t', '')
                .replace('\n', '');

            // store this HTML
            languages[urlConf.lang] = html;
        }
    });

    // write out all languages to file
    let data = {
        dateScraped: moment().format('YYYY-MM-DD-HH-mm-ss'),
        url: canonicalUrl,
        html: languages
    };

    const urlFilename = urlToFilename(canonicalUrl) + '.json';
    const pagePath = path.join(scrapedDataDir, urlFilename);
    const pageData = JSON.stringify(data, null, 4);
    saveFile(pagePath, pageData);
};

const getPageHTML = (url) => {

    let urls = [{
        url: url,
        lang: 'en'
    }];

    // do we need to make a Welsh URL too?
    if (url.indexOf('/welsh/') === -1) {
        let welshUrl = url.replace('https://31.221.8.237/', 'https://31.221.8.237/welsh/');
        urls.push({
            url: welshUrl,
            lang: 'cy'
        });
    }

    // create a list of scrape promises (english + welsh)
    let scrapeUrls = urls.map(u => scrapeUrl(u));

    // when they finish, store their HTML output
    pSettle(scrapeUrls).then(responses => storeContent(responses));
};

const saveFile = (filePath, content) => {
    try {
        fs.writeFileSync(filePath, content);
    } catch (err) {
        return console.error(`Error saving ${filePath}`, err);
    }
};

// take ugly HTML and pull out just the content we want
const parseContentFromHtml = (htmlLangs, url) => {

    let outputHtml = '';
    let divider = '=================================================================';

    for (let lang in htmlLangs) {
        let html = htmlLangs[lang];

        const { document } = new JSDOM(html).window;

        let contentParts = [
            document.querySelector('#titleBar'),
            document.querySelector('#mainContentContainer')
        ];

        if (contentParts[0].length === 0 || contentParts[1].length === 0) {
            console.log(`Error: elements not found for URL ${url}`);
        }

        let attributesToRemove = [
            'class',
            'style',
            'alt',
            'title',
            'id',
            'onclick',
            'align',
            'cellspacing',
            'cellpadding',
            'border',
            'target',
        ];

        const validEmptyElements = [
            'iframe',
            'img'
        ];

        const invalidEmptyElements = [
            'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'strong', 'b', 'em', 'i'
        ];

        // pairs of search/replace tag names
        const html5Fixes = [
            ['b', 'strong'],
            ['i', 'em']
        ];

        // clean up HTML
        contentParts.forEach(content => {

            function removeElement(el) {
                el.parentNode.removeChild(el);
            }

            function removeFromContent(selector) {
                forEach(content.querySelectorAll(selector), removeElement);
            }

            // remove unwanted attributes
            attributesToRemove.forEach(attr => {
                let elms = content.querySelectorAll(`[${attr}]`);
                forEach(elms, function (elm) {
                    elm.removeAttribute(attr);
                });
            });

            // remove non-breaking spaces in otherwise-empty elements
            invalidEmptyElements.forEach(tagName => {
                forEach(content.querySelectorAll(tagName), function (tag) {
                    if (tag.innerHTML.replace(/\s|&nbsp;|&#xA0;/g, '').length === 0) {
                        removeElement(tag);
                    }
                });
            });

            // use semantic content
            html5Fixes.forEach(fix => {
                const [search, replacement] = fix;
                forEach(document.querySelectorAll(search), el => {
                    el.outerHTML = `<${replacement}>${el.innerHTML}</${replacement}>`;
                });
            });

            // remove empty elements
            forEach(content.querySelectorAll(':empty'), el => {
                const elmType = el.name;
                if (validEmptyElements.indexOf(elmType) === -1) {
                    removeElement(el);
                }
            });

            // Remove embedded <style> or <script> tags
            removeFromContent('style');
            removeFromContent('script');
        });

        let title = contentParts[0].querySelector('h1');
        let subtitle = contentParts[0].querySelector('h2');

        if (title) { title = he.decode(title.innerHTML); }
        if (subtitle) { subtitle = he.decode(subtitle.innerHTML); }

        /**
         * Stuff to clean
         *
         * accordion divs
         * PDF downloads
         *
         */

        // prettify the text/html
        let bodyHtml = contentParts[1].innerHTML;

        // remove pointless <div> tags
        bodyHtml = bodyHtml.replace(/<\/?div>/g, '');
        let body = pretty(bodyHtml, { ocd: true });

        // convert `Yngl&#x177;n &#xE2;&#x2019;r` into `Ynglŷn â’r`
        html = he.decode(html);

        // build final HTML string
        let pagePath = url.replace('https://31.221.8.237/', '');
        let cleanHtml = `${divider}\n${pagePath}\n${divider}\n`;
        if (title) {
            cleanHtml += `${title}`;
        }
        if (subtitle) {
            cleanHtml += `\n${divider} \n${subtitle}`;
        }
        cleanHtml += `\n${divider} \n\n ${body} \n\n${divider} \n\n`;

        outputHtml += cleanHtml;
    }

    // write it to a file
    const urlFilename = urlToFilename(url) + '.html';
    const pagePath = path.join(cleanedDataDir, urlFilename);
    saveFile(pagePath, outputHtml);
};

// load a single HTML file and parse it
let loadAndParseHtml = filename => {
    let content;
    try {
        content = JSON.parse(fs.readFileSync(path.join(scrapedDataDir, filename), 'utf8'));
    } catch (e) {
        console.log('Could not parse JSON from file: ' + filename);
        console.log('Maybe you need to scrape this URL first?');
        process.exit(1);
    }
    parseContentFromHtml(content.html, content.url);
};

let loadJsonFiles = async (mode = 'parse') => {
    // parse HTML
    const files = fs.readdirSync(scrapedDataDir);
    let urls = [];

    files.forEach(filename => {
        if (mode === 'parse') {
            loadAndParseHtml(filename);
        } else if (mode === 'scrape') {
            let content;
            try {
                content = JSON.parse(fs.readFileSync(path.join(scrapedDataDir, filename), 'utf8'));
            } catch (e) {
                console.log('Could not parse JSON from file: ' + filename);
                console.log('Maybe you need to scrape this URL first?');
                process.exit(1);
            }
            urls.push(content.url);
        }
    });
    if (mode === 'scrape') {
        return urls;
    }
};

const init = async () => {

    // scrape pages
    if (argv.s) {
        if (!argv.u) {

            // First check which URLs have already been scraped
            let scrapedUrls = await loadJsonFiles('scrape');
            const urlsToScrape = difference(urls, scrapedUrls);
            console.log(`We've already scraped ${scrapedUrls.length} / ${urls.length} URLs, so scraping the remaining ${urlsToScrape.length}...`);

            // scrape URLs
            let timeout = 250;
            urlsToScrape.forEach((url, i) => {
                setTimeout(() => {
                    getPageHTML(url);
                }, timeout * i);
            });
        } else {
            // scrape a single (passed) URL
            getPageHTML(argv.u, true);
        }
    } else if (argv.p) {
        if (!argv.u) {
            // traverse scraped directory to parse HTML
            loadJsonFiles();
        } else {
            // load a URL
            let filename = urlToFilename(argv.u) + '.json';
            loadAndParseHtml(filename);
        }
    }

}

init();